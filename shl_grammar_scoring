{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":97919,"databundleVersionId":11694977,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install pandas numpy speechrecognition language-tool-python librosa scikit-learn matplotlib seaborn nltk","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SHL Hiring Assessment: Grammar Scoring Engine for Spoken Audios\n\n## Objective\nThe goal is to build a model that predicts a grammar score (0â€“5) for spoken audio samples based on their grammatical accuracy. The process involves:\n- Transcribing audio files to text.\n- Analyzing grammar errors in the transcriptions.\n- Extracting features and training a regression model.\n- Evaluating performance using Pearson Correlation and visualizations.\n\n## Approach\n1. Preprocess audio: Convert `.wav` files to text using Google's Speech-to-Text API.\n2. Extract features: Use grammar error counts, word count, sentence count, and average sentence length.\n3. Model: Train a tuned Random Forest Regressor and compare with a baseline model.\n4. Evaluate: Compute metrics (Pearson Correlation, MSE) and visualize results.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport speech_recognition as sr\nimport language_tool_python\nimport librosa\nimport nltk\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import pearsonr\n\n# Download NLTK data\nnltk.download(\"punkt\")\n\n# Initialize tools\nrecognizer = sr.Recognizer()\ngrammar_tool = language_tool_python.LanguageTool(\"en-US\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T16:15:55.454452Z","iopub.execute_input":"2025-04-05T16:15:55.454871Z","iopub.status.idle":"2025-04-05T16:16:09.366528Z","shell.execute_reply.started":"2025-04-05T16:15:55.454836Z","shell.execute_reply":"2025-04-05T16:16:09.364190Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load datasets\ntrain_df = pd.read_csv(\"/kaggle/input/shl-intern-hiring-assessment/dataset/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/shl-intern-hiring-assessment/dataset/test.csv\")\nsample_submission = pd.read_csv(\"/kaggle/input/shl-intern-hiring-assessment/dataset/sample_submission.csv\")\n\n# Display basic info\nprint(\"Training Data Shape:\", train_df.shape)\nprint(\"Test Data Shape:\", test_df.shape)\nprint(\"\\nTraining Data Sample:\")\nprint(train_df.head())\n\n# Visualize grammar score distribution\nplt.figure(figsize=(8, 6))\nsns.histplot(train_df[\"label\"], bins=10, kde=True)  # Changed from \"grammar_score\" to \"label\"\nplt.title(\"Distribution of Grammar Scores (Training Set)\")\nplt.xlabel(\"Grammar Score (0-5)\")\nplt.ylabel(\"Frequency\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T16:16:09.872089Z","iopub.execute_input":"2025-04-05T16:16:09.872430Z","iopub.status.idle":"2025-04-05T16:16:10.171312Z","shell.execute_reply.started":"2025-04-05T16:16:09.872403Z","shell.execute_reply":"2025-04-05T16:16:10.169748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define paths (for Kaggle environment)\ntrain_audio_dir = \"/kaggle/input/shl-intern-hiring-assessment/dataset/audios_train/\"\ntest_audio_dir = \"/kaggle/input/shl-intern-hiring-assessment/dataset/audios_test/\"\n\n# Function to transcribe audio with error handling\ndef transcribe_audio(audio_path):\n    try:\n        with sr.AudioFile(audio_path) as source:\n            audio = recognizer.record(source, duration=60)  # Limit to 60 seconds to match file length\n            text = recognizer.recognize_google(audio)\n            return text\n    except (sr.UnknownValueError, sr.RequestError) as e:\n        print(f\"Transcription failed for {audio_path}: {e}\")\n        return \"\"\n    except FileNotFoundError:\n        print(f\"File not found: {audio_path}\")\n        return \"\"\n\n# Debug: Verify directory contents\nprint(\"Training audio files in directory:\")\nprint(os.listdir(train_audio_dir)[:5])  # Print first 5 files to verify\nprint(\"Test audio files in directory:\")\nprint(os.listdir(test_audio_dir)[:5])  # Print first 5 files to verify\n\n# Transcribe training audio files\ntrain_transcriptions = {}\nfor filename in train_df[\"filename\"]:\n    audio_path = os.path.join(train_audio_dir, filename)\n    print(f\"Transcribing training file: {audio_path}\")  # Debug print\n    if os.path.exists(audio_path):\n        train_transcriptions[filename] = transcribe_audio(audio_path)\n    else:\n        print(f\"Warning: {audio_path} not found!\")\n        train_transcriptions[filename] = \"\"\n\n# Transcribe test audio files\ntest_transcriptions = {}\nfor filename in test_df[\"filename\"]:\n    audio_path = os.path.join(test_audio_dir, filename)\n    print(f\"Transcribing test file: {audio_path}\")  # Debug print\n    if os.path.exists(audio_path):\n        test_transcriptions[filename] = transcribe_audio(audio_path)\n    else:\n        print(f\"Warning: {audio_path} not found!\")\n        test_transcriptions[filename] = \"\"\n\n# Add transcriptions to dataframes\ntrain_df[\"transcription\"] = train_df[\"filename\"].map(train_transcriptions)\ntest_df[\"transcription\"] = test_df[\"filename\"].map(test_transcriptions)\n\n# Save transcriptions to avoid reprocessing\ntrain_df.to_csv(\"/kaggle/working/train_with_transcriptions.csv\", index=False)\ntest_df.to_csv(\"/kaggle/working/test_with_transcriptions.csv\", index=False)\n\n# Verify transcription results\nprint(\"\\nSample transcriptions from training data:\")\nprint(train_df[[\"filename\", \"transcription\"]].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:31:54.995316Z","iopub.execute_input":"2025-04-05T17:31:54.995787Z","iopub.status.idle":"2025-04-05T20:44:34.859686Z","shell.execute_reply.started":"2025-04-05T17:31:54.995749Z","shell.execute_reply":"2025-04-05T20:44:34.857008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_grammar_errors(text):\n    if not text or len(text.strip()) == 0:\n        return 10\n    matches = grammar_tool.check(text)\n    return len(matches)\n\ndef extract_features(text):\n    if not text or len(text.strip()) == 0:\n        return 0, 0, 0, 0\n    words = text.split()\n    word_count = len(words)\n    sentences = nltk.sent_tokenize(text)\n    sentence_count = len(sentences)\n    avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0\n    error_count = get_grammar_errors(text)\n    return word_count, sentence_count, avg_sentence_length, error_count\n\ntrain_df[[\"word_count\", \"sentence_count\", \"avg_sentence_length\", \"error_count\"]] = pd.DataFrame(\n    train_df[\"transcription\"].apply(extract_features).tolist(), index=train_df.index\n)\ntest_df[[\"word_count\", \"sentence_count\", \"avg_sentence_length\", \"error_count\"]] = pd.DataFrame(\n    test_df[\"transcription\"].apply(extract_features).tolist(), index=test_df.index\n)\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(train_df[[\"label\", \"error_count\", \"word_count\", \"sentence_count\", \"avg_sentence_length\"]].corr(), annot=True, cmap=\"coolwarm\")\nplt.title(\"Correlation Matrix of Features\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T20:56:54.253606Z","iopub.execute_input":"2025-04-05T20:56:54.254057Z","iopub.status.idle":"2025-04-05T21:13:32.393831Z","shell.execute_reply.started":"2025-04-05T20:56:54.254023Z","shell.execute_reply":"2025-04-05T21:13:32.392308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare features and target for modeling\nfeatures = [\"error_count\", \"word_count\", \"sentence_count\", \"avg_sentence_length\"]\nX = train_df[features]\ny = train_df[\"label\"]\n\n# Split data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Baseline: Average score by error count bin\ntrain_df[\"error_bin\"] = pd.qcut(train_df[\"error_count\"], q=5, labels=False, duplicates=\"drop\")\nbaseline_scores = train_df.groupby(\"error_bin\")[\"label\"].mean()\nbins = pd.qcut(train_df[\"error_count\"], q=5, retbins=True, duplicates=\"drop\")[1]\ndef baseline_predict(error_count):\n    bin_idx = pd.cut([error_count], bins=bins, labels=False, include_lowest=True)[0]\n    if pd.isna(bin_idx):\n        return baseline_scores.mean()\n    return baseline_scores[bin_idx]\n\ny_pred_baseline = [baseline_predict(ec) for ec in X_val[\"error_count\"]]\npearson_baseline, _ = pearsonr(y_val, y_pred_baseline)\nprint(f\"Baseline Pearson Correlation: {pearson_baseline:.3f}\")\n\n# Train tuned Random Forest model\nmodel = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_val)\n\n# Evaluate model performance\npearson_corr, _ = pearsonr(y_val, y_pred)\nmse = mean_squared_error(y_val, y_pred)\nprint(f\"Validation Pearson Correlation: {pearson_corr:.3f}\")\nprint(f\"Validation Mean Squared Error: {mse:.3f}\")\n\n# Visualize predictions vs true values\nplt.figure(figsize=(10, 8))\nplt.scatter(y_val, y_pred_baseline, alpha=0.5, label=\"Baseline Prediction\")\nplt.scatter(y_val, y_pred, alpha=0.5, label=\"Random Forest Prediction\")\nplt.plot([0, 5], [0, 5], \"r--\", label=\"Perfect Prediction\")\nplt.xlabel(\"True Grammar Score\")\nplt.ylabel(\"Predicted Grammar Score\")\nplt.title(\"Predicted vs True Grammar Scores (Validation Set)\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T21:17:12.100671Z","iopub.execute_input":"2025-04-05T21:17:12.101242Z","iopub.status.idle":"2025-04-05T21:17:12.857182Z","shell.execute_reply.started":"2025-04-05T21:17:12.101203Z","shell.execute_reply":"2025-04-05T21:17:12.855473Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate predictions for test set\nX_test = test_df[features]\ntest_df[\"grammar_score\"] = model.predict(X_test)\nsubmission = test_df[[\"filename\", \"grammar_score\"]]\nsubmission.to_csv(\"/kaggle/working/submission.csv\", index=False)\nprint(\"Sample Submission:\")\nprint(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T21:17:17.973984Z","iopub.execute_input":"2025-04-05T21:17:17.974432Z","iopub.status.idle":"2025-04-05T21:17:18.005470Z","shell.execute_reply.started":"2025-04-05T21:17:17.974400Z","shell.execute_reply":"2025-04-05T21:17:18.004119Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Conclusion\n\n### Approach Summary\n- **Preprocessing**: Transcribed 444 training and 195 test audio files using Google's Speech-to-Text API, with verified accuracy.\n- **Feature Extraction**: Extracted grammar error counts, word count, sentence count, and average sentence length.\n- **Model**: Used a tuned Random Forest Regressor (200 trees, max depth 10) and compared with a baseline model averaging scores by error bins.\n- **Evaluation**: Achieved a Pearson Correlation of 0.378 on the validation set, outperforming the baseline (0.106).\n\n### Performance\n- The Random Forest model shows moderate predictive power, with an MSE of 1.217, indicating reasonable accuracy. The baseline provides a reference point, highlighting the model's value.\n\n## Submission\nThe final predictions are saved in `/kaggle/working/submission.csv`.","metadata":{}}]}